{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d614c4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importacion de librerias\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from tensorflow.python.ops import math_ops\n",
    "import datetime\n",
    "import random\n",
    "from keras.callbacks import EarlyStopping\n",
    "import scipy.stats as stats\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46da20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa el conjunto de datos \n",
    "\n",
    "events_container_path= \"C:\\\\TESIS\\\\FINAL\\\\base_datos\\\\\"\n",
    "\n",
    "dataset1 = pd.read_pickle(events_container_path + \"rcc_earthquake_database_2011_2022_v5_1.pkl\")      \n",
    "dataset2 = pd.read_pickle(events_container_path + \"rcc_earthquake_database_2023_v5_1.pkl\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5864ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset1.shape)\n",
    "print(dataset2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93337061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenamos los datasets\n",
    "dataset = pd.concat([dataset1, dataset2])\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab305231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En los datos existen indistintamente  Azimuths nombrados como  0 y 360 se unifican como 0\n",
    "\n",
    "dataset['Azimuth']= dataset['Azimuth'].replace(360,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde07b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan filas que no cumplen con el tiempo de la ventana de datos\n",
    "\n",
    "count = 0\n",
    "max_window = 501\n",
    "to_delete=[]\n",
    "initial_length= len(dataset)\n",
    "print ('Tamaño inicial :', initial_length)\n",
    "for i in range(initial_length):\n",
    "  tmp=dataset.iloc[i]\n",
    "  z= tmp['Z_data']\n",
    "  n= tmp['N_data']\n",
    "  e= tmp['E_data'] \n",
    "  if (len(z) != max_window) or (len(n) != max_window) or (len(e) != max_window):   \n",
    "    to_delete.append(i)\n",
    "    count+=1\n",
    "print('to_delete rows',to_delete)\n",
    "dataset.drop(to_delete, axis=0, inplace=True)\n",
    "print ('Eliminadas ',count,' filas')\n",
    "print(len(dataset))\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634cb769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Restablezco los indices del dataset eliminando las discontinuidades por las filas eliminadas\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16893123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscamos el maximo valor absoluto de los datos porque existe un dato con un valor maximo erroneo\n",
    "n =0\n",
    "global_max = float('-inf')\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    row = dataset.iloc[i]  # Obtén la fila actual\n",
    "    \n",
    "    for channel in ['Z_data', 'N_data', 'E_data']:\n",
    "        channel_values = row[channel]  # Obtén el arreglo correspondiente al canal actual\n",
    "        \n",
    "        channel_max = np.max(np.abs(channel_values))  # Encuentra el valor máximo absoluto en el arreglo\n",
    "        \n",
    "        # Actualiza el máximo global si es necesario\n",
    "        if channel_max > global_max:\n",
    "            global_max = channel_max\n",
    "            n=i\n",
    "\n",
    "print(\"Valor máximo absoluto:\", global_max)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba745d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizo el dato erroneo\n",
    "dataset.iloc[442]['RCC_OBSPY'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d453611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimino esta fila del dataset que posee problemas  con el valor de los canales y me altera el programa al calcular el maximo\n",
    "dataset = dataset.drop(dataset.index[442])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cc1c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restablezco los indices del dataset eliminando las discontinuidades por las filas eliminadas\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c2ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se convierte a float las columnas que asi lo requieren\n",
    "\n",
    "dataset['Mc']= dataset['Mc'].astype(float, errors = 'raise')\n",
    "dataset['Azimuth']= dataset['Azimuth'].astype(float, errors = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0110b01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convertimos los grados a radianes\n",
    "dataset['Azimuth_radianes']= [math.radians(i) for i in dataset['Azimuth']]   \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1420ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conservo el dataset original\n",
    "dataset_original= dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cbe923",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9dc963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino los eventos a mas de  110Km de distancia (1 grado)\n",
    "\n",
    "indexNames = dataset[ (dataset['Distance'] > 110)].index\n",
    "dataset.drop(indexNames , inplace=True)\n",
    "#sns.pairplot(dataset[['Azimuth','Distance','Mc','Depth']], diag_kind=\"kde\")\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad58ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Crear nuevos DataFrames para cada condición de SNR\n",
    "dataset_snr25 = dataset[(dataset['SNR_Z_S'] >= 25) & (dataset['SNR_N_S'] >= 25) & (dataset['SNR_E_S'] >= 25)].copy()\n",
    "\n",
    "dataset_snr20 = dataset[(dataset['SNR_Z_S'] >= 20) & (dataset['SNR_N_S'] >= 20) & (dataset['SNR_E_S'] >= 20)\n",
    "                        & (dataset['SNR_Z_S'] < 25) & (dataset['SNR_N_S'] < 25) & (dataset['SNR_E_S'] < 25)].copy()\n",
    "dataset_snr18 = dataset[(dataset['SNR_Z_S'] >= 18) & (dataset['SNR_N_S'] >= 18) & (dataset['SNR_E_S'] >= 18)\n",
    "                        & (dataset['SNR_Z_S'] < 20) & (dataset['SNR_N_S'] < 20) & (dataset['SNR_E_S'] < 20)].copy()\n",
    "\n",
    "dataset_snr15 = dataset[(dataset['SNR_Z_S'] >= 15) & (dataset['SNR_N_S'] >= 15) & (dataset['SNR_E_S'] >= 15)\n",
    "                        & (dataset['SNR_Z_S'] < 18) & (dataset['SNR_N_S'] < 18) & (dataset['SNR_E_S'] < 18)].copy()\n",
    "\n",
    "dataset_snr10 = dataset[(dataset['SNR_Z_S'] >= 10) & (dataset['SNR_N_S'] >= 10) & (dataset['SNR_E_S'] >= 10)\n",
    "                        & (dataset['SNR_Z_S'] < 15) & (dataset['SNR_N_S'] < 15) & (dataset['SNR_E_S'] < 15)].copy()\n",
    "\n",
    "dataset_snr5 = dataset[(dataset['SNR_Z_S'] >= 5) & (dataset['SNR_N_S'] >= 5) & (dataset['SNR_E_S'] >= 5)\n",
    "                       & (dataset['SNR_Z_S'] < 10) & (dataset['SNR_N_S'] < 10) & (dataset['SNR_E_S'] < 10)].copy()\n",
    "\n",
    "dataset_snr0 = dataset[(dataset['SNR_Z_S'] > 0) & (dataset['SNR_N_S'] > 0) & (dataset['SNR_E_S'] > 0)\n",
    "                       & (dataset['SNR_Z_S'] < 5) & (dataset['SNR_N_S'] < 5) & (dataset['SNR_E_S'] < 5)].copy()\n",
    "\n",
    "# Filtrar los valores que cumplan alguna de las condiciones anteriores o tengan algún valor None en las columnas \n",
    "# para crear el dataset_invalid\n",
    "dataset_invalid = dataset[~(\n",
    "    (dataset['SNR_Z_S'] >= 25) & (dataset['SNR_N_S'] >= 25) & (dataset['SNR_E_S'] >= 25) |\n",
    "    ((dataset['SNR_Z_S'] >= 20) & (dataset['SNR_N_S'] >= 20) & (dataset['SNR_E_S'] >= 20) &\n",
    "    (dataset['SNR_Z_S'] < 25) & (dataset['SNR_N_S'] < 25) & (dataset['SNR_E_S'] < 25)) |\n",
    "    ((dataset['SNR_Z_S'] >= 18) & (dataset['SNR_N_S'] >= 18) & (dataset['SNR_E_S'] >= 18) &\n",
    "    (dataset['SNR_Z_S'] < 20) & (dataset['SNR_N_S'] < 20) & (dataset['SNR_E_S'] < 20)) |    \n",
    "    ((dataset['SNR_Z_S'] >= 15) & (dataset['SNR_N_S'] >= 15) & (dataset['SNR_E_S'] >= 15) &\n",
    "    (dataset['SNR_Z_S'] < 18) & (dataset['SNR_N_S'] < 18) & (dataset['SNR_E_S'] < 18)) |\n",
    "    ((dataset['SNR_Z_S'] >= 10) & (dataset['SNR_N_S'] >= 10) & (dataset['SNR_E_S'] >= 10) &\n",
    "    (dataset['SNR_Z_S'] < 15) & (dataset['SNR_N_S'] < 15) & (dataset['SNR_E_S'] < 15)) |\n",
    "    ((dataset['SNR_Z_S'] >= 5) & (dataset['SNR_N_S'] >= 5) & (dataset['SNR_E_S'] >= 5) &\n",
    "    (dataset['SNR_Z_S'] < 10) & (dataset['SNR_N_S'] < 10) & (dataset['SNR_E_S'] < 10)) |\n",
    "    ((dataset['SNR_Z_S'] > 0) & (dataset['SNR_N_S'] > 0) & (dataset['SNR_E_S'] > 0) &\n",
    "    (dataset['SNR_Z_S'] < 5) & (dataset['SNR_N_S'] < 5) & (dataset['SNR_E_S'] < 5))) &\n",
    "    (dataset['SNR_Z_S'].notna() & dataset['SNR_N_S'].notna() & dataset['SNR_E_S'].notna())].copy()\n",
    "\n",
    "# Crear el dataset_None con los elementos que tienen algún valor None en las columnas\n",
    "dataset_None = dataset[dataset['SNR_Z_S'].isna() | dataset['SNR_N_S'].isna() | dataset['SNR_E_S'].isna()].copy()\n",
    "\n",
    "# Marcar la columna 'Is_Valid' en cada DataFrame\n",
    "dataset_snr25['Is_Valid'] = True\n",
    "dataset_snr20['Is_Valid'] = False\n",
    "dataset_snr18['Is_Valid'] = False\n",
    "dataset_snr15['Is_Valid'] = False\n",
    "dataset_snr10['Is_Valid'] = False\n",
    "dataset_snr5['Is_Valid'] = False\n",
    "dataset_snr0['Is_Valid'] = False\n",
    "dataset_invalid['Is_Valid'] = False\n",
    "dataset_None['Is_Valid'] = False\n",
    "\n",
    "# Verificar los nuevos DataFrames\n",
    "print(\"Dataset SNR >= 25:\")\n",
    "print(dataset_snr25.shape)\n",
    "\n",
    "print(\"\\nDataset SNR >= 20:\")\n",
    "print(dataset_snr20.shape)\n",
    "\n",
    "print(\"\\nDataset SNR >= 18:\")\n",
    "print(dataset_snr18.shape)\n",
    "\n",
    "print(\"\\nDataset SNR >= 15:\")\n",
    "print(dataset_snr15.shape)\n",
    "\n",
    "print(\"\\nDataset SNR >= 10:\")\n",
    "print(dataset_snr10.shape)\n",
    "\n",
    "print(\"\\nDataset SNR >= 5:\")\n",
    "print(dataset_snr5.shape)\n",
    "\n",
    "print(\"\\nDataset SNR > 0:\")\n",
    "print(dataset_snr0.shape)\n",
    "\n",
    "print(\"\\nDataset Invalid:\")\n",
    "print(dataset_invalid.shape)\n",
    "\n",
    "print(\"\\nDataset None:\")\n",
    "print(dataset_None.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dcf9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restablezco los indices del dataset eliminando las discontinuidades por las filas eliminadas\n",
    "dataset_snr25 = dataset_snr25.reset_index(drop=True)\n",
    "dataset_snr20 = dataset_snr20.reset_index(drop=True)\n",
    "dataset_snr18 = dataset_snr18.reset_index(drop=True)\n",
    "dataset_snr15 = dataset_snr15.reset_index(drop=True)\n",
    "dataset_snr10 = dataset_snr10.reset_index(drop=True)\n",
    "dataset_snr5 = dataset_snr5.reset_index(drop=True)\n",
    "dataset_snr0 = dataset_snr0.reset_index(drop=True)\n",
    "dataset_invalid = dataset_invalid.reset_index(drop=True)\n",
    "dataset_None = dataset_None.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729708c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Busco en el dataset_invalid aquellos datos que cumplen la SNR por cada canal independiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23096ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_snr_any_channel = dataset_invalid[((dataset_invalid['SNR_Z_S'] >= 15) & \n",
    "                                           ((dataset_invalid['SNR_N_S'] >= 10) &\n",
    "                                           (dataset_invalid['SNR_E_S'] >= 10)))|\n",
    "                                           ((dataset_invalid['SNR_N_S'] >= 15) & \n",
    "                                           ((dataset_invalid['SNR_Z_S'] >= 10) & \n",
    "                                           (dataset_invalid['SNR_E_S'] >= 10)))|\n",
    "                                           ((dataset_invalid['SNR_E_S'] >= 15) &\n",
    "                                           ((dataset_invalid['SNR_Z_S'] >= 10) &\n",
    "                                           (dataset_invalid['SNR_N_S'] >= 10)))].copy()\n",
    "\n",
    "dataset_new_invalid = dataset_invalid[~(((dataset_invalid['SNR_Z_S'] >= 15) &\n",
    "                                       ((dataset_invalid['SNR_N_S'] >= 10) &\n",
    "                                       (dataset_invalid['SNR_E_S'] >= 10)))|\n",
    "                                       ((dataset_invalid['SNR_N_S'] >= 15) &\n",
    "                                       ((dataset_invalid['SNR_Z_S'] >= 10) &\n",
    "                                       (dataset_invalid['SNR_E_S'] >= 10)))|\n",
    "                                       ((dataset_invalid['SNR_E_S'] >= 15) &\n",
    "                                       ((dataset_invalid['SNR_Z_S'] >= 10) & (dataset_invalid['SNR_N_S'] >= 10))))].copy()\n",
    "\n",
    "\n",
    "\n",
    "pd.options.display.max_rows = 20\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "#Restablezco los indices del dataset eliminando las discontinuidades por las filas eliminadas\n",
    "dataset_snr_any_channel = dataset_snr_any_channel.reset_index(drop=True)\n",
    "dataset_new_invalid = dataset_new_invalid.reset_index(drop=True)\n",
    "# Verificar los nuevos DataFrames\n",
    "print(\"Dataset SNR Any Channel :\")\n",
    "print(dataset_snr_any_channel.shape)\n",
    "\n",
    "print(\"Dataset SNR New Invalid:\")\n",
    "print(dataset_new_invalid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db19f14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_snr_any_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trabajaremos con los dataset_None pero utilizando la SNR de P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40be2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuevos DataFrames None para cada condición\n",
    "dataset_None_snr25 = dataset_None[(dataset_None['SNR_Z_P'] >= 25) & (dataset_None['SNR_N_P'] >= 25) &\n",
    "                                  (dataset_None['SNR_E_P'] >= 25)].copy()\n",
    "\n",
    "dataset_None_snr20 = dataset_None[(dataset_None['SNR_Z_P'] >= 20) & (dataset_None['SNR_N_P'] >= 20) &\n",
    "                                  (dataset_None['SNR_E_P'] >= 20) &\n",
    "                                  (dataset_None['SNR_Z_P'] < 25) &\n",
    "                                  (dataset_None['SNR_N_P'] < 25) & (dataset_None['SNR_E_P'] < 25)].copy()\n",
    "dataset_None_snr18 = dataset_None[(dataset_None['SNR_Z_P'] >= 18) &\n",
    "                                  (dataset_None['SNR_N_P'] >= 18) & (dataset_None['SNR_E_P'] >= 18) &\n",
    "                                  (dataset_None['SNR_Z_P'] < 20) &\n",
    "                                  (dataset_None['SNR_N_P'] < 20) &\n",
    "                                  (dataset_None['SNR_E_P'] < 20)].copy()\n",
    "\n",
    "dataset_None_snr15 = dataset_None[(dataset_None['SNR_Z_P'] >= 15) & (dataset_None['SNR_N_P'] >= 15) &\n",
    "                                  (dataset_None['SNR_E_P'] >= 15) &\n",
    "                                  (dataset_None['SNR_Z_P'] < 18) &\n",
    "                                  (dataset_None['SNR_N_P'] < 18) &\n",
    "                                  (dataset_None['SNR_E_P'] < 18)].copy()\n",
    "\n",
    "dataset_None_snr10 = dataset_None[(dataset_None['SNR_Z_P'] >= 10) & (dataset_None['SNR_N_P'] >= 10) &\n",
    "                                  (dataset_None['SNR_E_P'] >= 10) & (dataset_None['SNR_Z_P'] < 15) &\n",
    "                                  (dataset_None['SNR_N_P'] < 15) & (dataset_None['SNR_E_P'] < 15)].copy()\n",
    "\n",
    "dataset_None_snr5 = dataset_None[(dataset_None['SNR_Z_P'] >= 5) & (dataset_None['SNR_N_P'] >= 5) &\n",
    "                                 (dataset_None['SNR_E_P'] >= 5) & (dataset_None['SNR_Z_P'] < 10) &\n",
    "                                 (dataset_None['SNR_N_P'] < 10) & (dataset_None['SNR_E_P'] < 10)].copy()\n",
    "\n",
    "dataset_None_snr0 = dataset_None[(dataset_None['SNR_Z_P'] > 0) & (dataset_None['SNR_N_P'] > 0) &\n",
    "                                 (dataset_None['SNR_E_P'] > 0) & (dataset_None['SNR_Z_P'] < 5) &\n",
    "                                 (dataset_None['SNR_N_P'] < 5) & (dataset_None['SNR_E_P'] < 5)].copy()\n",
    "\n",
    "# Filtrar los valores que cumplan alguna de las condiciones anteriores o tengan algún valor None en las columnas\n",
    "dataset_None_invalid = dataset_None[~(\n",
    "    (dataset_None['SNR_Z_P'] >= 25) & (dataset_None['SNR_N_P'] >= 25) & (dataset_None['SNR_E_P'] >= 25) |\n",
    "    ((dataset_None['SNR_Z_P'] >= 20) & (dataset_None['SNR_N_P'] >= 20) & (dataset_None['SNR_E_P'] >= 20) &\n",
    "    (dataset_None['SNR_Z_P'] < 25) & (dataset_None['SNR_N_P'] < 25) & (dataset_None['SNR_E_P'] < 25)) |\n",
    "    ((dataset_None['SNR_Z_P'] >= 18) & (dataset_None['SNR_N_P'] >= 18) & (dataset_None['SNR_E_P'] >= 18) &\n",
    "    (dataset_None['SNR_Z_P'] < 20) & (dataset_None['SNR_N_P'] < 20) & (dataset_None['SNR_E_P'] < 20)) |    \n",
    "    ((dataset_None['SNR_Z_P'] >= 15) & (dataset_None['SNR_N_P'] >= 15) & (dataset_None['SNR_E_P'] >= 15) &\n",
    "    (dataset_None['SNR_Z_P'] < 18) & (dataset_None['SNR_N_P'] < 18) & (dataset_None['SNR_E_P'] < 18)) |\n",
    "    ((dataset_None['SNR_Z_P'] >= 10) & (dataset_None['SNR_N_P'] >= 10) & (dataset_None['SNR_E_P'] >= 10) &\n",
    "    (dataset_None['SNR_Z_P'] < 15) & (dataset_None['SNR_N_P'] < 15) & (dataset_None['SNR_E_P'] < 15)) |\n",
    "    ((dataset_None['SNR_Z_P'] >= 5) & (dataset_None['SNR_N_P'] >= 5) & (dataset_None['SNR_E_P'] >= 5) &\n",
    "    (dataset_None['SNR_Z_P'] < 10) & (dataset_None['SNR_N_P'] < 10) & (dataset_None['SNR_E_P'] < 10)) |\n",
    "    ((dataset_None['SNR_Z_P'] > 0) & (dataset_None['SNR_N_P'] > 0) & (dataset_None['SNR_E_P'] > 0) &\n",
    "    (dataset_None['SNR_Z_P'] < 5) & (dataset_None['SNR_N_P'] < 5) & (dataset_None['SNR_E_P'] < 5)))].copy()\n",
    "\n",
    "\n",
    "\n",
    "# Marcar la columna 'Is_Valid' en cada DataFrame\n",
    "dataset_None_snr25['Is_Valid'] = True\n",
    "dataset_None_snr20['Is_Valid'] = False\n",
    "dataset_None_snr18['Is_Valid'] = False\n",
    "dataset_None_snr15['Is_Valid'] = False\n",
    "dataset_None_snr10['Is_Valid'] = False\n",
    "dataset_None_snr5['Is_Valid'] = False\n",
    "dataset_None_snr0['Is_Valid'] = False\n",
    "dataset_None_invalid['Is_Valid'] = False\n",
    "\n",
    "# Verificar los nuevos DataFrames\n",
    "print(\"Dataset None SNR >= 25:\")\n",
    "print(dataset_None_snr25.shape)\n",
    "\n",
    "print(\"\\nDataset None SNR >= 20:\")\n",
    "print(dataset_None_snr20.shape)\n",
    "\n",
    "print(\"\\nDataset None SNR >= 18:\")\n",
    "print(dataset_None_snr18.shape)\n",
    "\n",
    "print(\"\\nDataset None SNR >= 15:\")\n",
    "print(dataset_None_snr15.shape)\n",
    "\n",
    "print(\"\\nDataset NoneSNR >= 10:\")\n",
    "print(dataset_None_snr10.shape)\n",
    "\n",
    "print(\"\\nDataset None SNR >= 5:\")\n",
    "print(dataset_None_snr5.shape)\n",
    "\n",
    "print(\"\\nDataset None SNR > 0:\")\n",
    "print(dataset_None_snr0.shape)\n",
    "\n",
    "print(\"\\nDataset None Invalid:\")\n",
    "print(dataset_None_invalid.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f507a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restablezco los indices del dataset eliminando las discontinuidades por las filas eliminadas\n",
    "dataset_None_snr25 = dataset_None_snr25.reset_index(drop=True)\n",
    "dataset_None_snr20 = dataset_None_snr20.reset_index(drop=True)\n",
    "dataset_None_snr18 = dataset_None_snr18.reset_index(drop=True)\n",
    "dataset_None_snr15 = dataset_None_snr15.reset_index(drop=True)\n",
    "dataset_None_snr10 = dataset_None_snr10.reset_index(drop=True)\n",
    "dataset_None_snr5 = dataset_None_snr5.reset_index(drop=True)\n",
    "dataset_None_snr0 = dataset_None_snr0.reset_index(drop=True)\n",
    "dataset_None_invalid = dataset_None_invalid.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9141cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_None_snr_any_channel = dataset_None_invalid[((dataset_None_invalid['SNR_Z_P'] >= 15) &\n",
    "                                                     ((dataset_None_invalid['SNR_N_P'] >= 10) &\n",
    "                                                     (dataset_None_invalid['SNR_E_P'] >= 10)))|\n",
    "                                                     ((dataset_None_invalid['SNR_N_P'] >= 15) &\n",
    "                                                     ((dataset_None_invalid['SNR_Z_P'] >= 10) &\n",
    "                                                     (dataset_None_invalid['SNR_E_P'] >= 10)))|\n",
    "                                                     ((dataset_None_invalid['SNR_E_P'] >= 15) &\n",
    "                                                     ((dataset_None_invalid['SNR_Z_P'] >= 10) &\n",
    "                                                     (dataset_None_invalid['SNR_N_P'] >= 10)))].copy()\n",
    "\n",
    "dataset_None_new_invalid = dataset_None_invalid[~(((dataset_None_invalid['SNR_Z_P'] >= 15) &\n",
    "                                                  ((dataset_None_invalid['SNR_N_P'] >= 10) &\n",
    "                                                  (dataset_None_invalid['SNR_E_P'] >= 10)))| \n",
    "                                                  ((dataset_None_invalid['SNR_N_P'] >= 15) &\n",
    "                                                  ((dataset_None_invalid['SNR_Z_P'] >= 10) &\n",
    "                                                  (dataset_None_invalid['SNR_E_P'] >= 10)))| \n",
    "                                                  ((dataset_None_invalid['SNR_E_P'] >= 15) &\n",
    "                                                  ((dataset_None_invalid['SNR_Z_P'] >= 10) &\n",
    "                                                  (dataset_None_invalid['SNR_N_P'] >= 10))))].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Restablezco los indices del dataset eliminando las discontinuidades por las filas eliminadas\n",
    "dataset_None_snr_any_channel = dataset_None_snr_any_channel.reset_index(drop=True)\n",
    "dataset_None_new_invalid = dataset_None_new_invalid.reset_index(drop=True)\n",
    "# Verificar los nuevos DataFrames\n",
    "print(\"Dataset SNR None Any Channel >= 15:\")\n",
    "print(dataset_None_snr_any_channel.shape)\n",
    "\n",
    "print(\"Dataset SNR None New Invalid:\")\n",
    "print(dataset_None_new_invalid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a3e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvo los datasets\n",
    "events_container_path= \"C:\\\\TESIS\\\\FINAL\\\\Training\\\\Preprocesing\\\\SNR_Datasets_validos\\\\\" \n",
    "dataset.to_pickle(events_container_path + 'dataset.pkl')\n",
    "dataset_snr25.to_pickle(events_container_path + 'dataset_snr25.pkl') \n",
    "dataset_snr20.to_pickle(events_container_path + 'dataset_snr20.pkl') \n",
    "dataset_snr18.to_pickle(events_container_path + 'dataset_snr18.pkl') \n",
    "dataset_snr15.to_pickle(events_container_path + 'dataset_snr15.pkl') \n",
    "dataset_snr10.to_pickle(events_container_path + 'dataset_snr10.pkl') \n",
    "dataset_snr5.to_pickle(events_container_path + 'dataset_snr5.pkl') \n",
    "dataset_snr0.to_pickle(events_container_path + 'dataset_snr0.pkl') \n",
    "dataset_invalid.to_pickle(events_container_path + 'dataset_invalid.pkl')\n",
    "dataset_snr_any_channel.to_pickle(events_container_path + 'dataset_snr_any_channel.pkl')\n",
    "dataset_new_invalid.to_pickle(events_container_path + 'dataset_new_invalid.pkl')\n",
    "\n",
    "dataset_None_snr25.to_pickle(events_container_path + 'dataset_None_snr25.pkl') \n",
    "dataset_None_snr20.to_pickle(events_container_path + 'dataset_None_snr20.pkl') \n",
    "dataset_None_snr18.to_pickle(events_container_path + 'dataset_None_snr18.pkl') \n",
    "dataset_None_snr15.to_pickle(events_container_path + 'dataset_None_snr15.pkl') \n",
    "dataset_None_snr10.to_pickle(events_container_path + 'dataset_None_snr10.pkl') \n",
    "dataset_None_snr5.to_pickle(events_container_path + 'dataset_None_snr5.pkl') \n",
    "dataset_None_snr0.to_pickle(events_container_path + 'dataset_None_snr0.pkl') \n",
    "dataset_None.to_pickle(events_container_path + 'dataset_None.pkl') \n",
    "dataset_None_invalid.to_pickle(events_container_path + 'dataset_None_invalid.pkl') \n",
    "dataset_None_snr_any_channel.to_pickle(events_container_path + 'dataset_None_snr_any_channel.pkl')\n",
    "dataset_None_new_invalid.to_pickle(events_container_path + 'dataset_None_new_invalid.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57146639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui comienza la segunda notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb4df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea el dataset final a partir de los datasets anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b20049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se mezcla en un solo dataset\n",
    "dataset_valid = pd.concat([dataset_snr25, dataset_snr20, dataset_snr18, dataset_snr15, dataset_snr_any_channel, \n",
    "                           dataset_None_snr25, dataset_None_snr20, dataset_None_snr15, dataset_None_snr10,\n",
    "                           dataset_None_snr_any_channel])\n",
    "dataset_valid = dataset_valid.reset_index(drop=True)\n",
    "dataset_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dbf86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faea00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Garantizo que la columna Is_valid este en True\n",
    "dataset_valid['Is_Valid'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d3e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se busca correlacion en los datos y posibles outliers\n",
    "\n",
    "sns.pairplot(dataset_valid[['Azimuth','Distance','Mc','Depth', 'SNR_Z_S', 'SNR_N_S',\n",
    "                            'SNR_E_S','SNR_Z_P', 'SNR_N_P', 'SNR_E_P']], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hace un histograma de la variable a predecir \"azimut\"\n",
    "frequencies, bins, _ = plt.hist(dataset_valid['Azimuth'], bins=20)\n",
    "plt.xlabel('Azimut')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "# Se calcula el valor medio de las frecuencias\n",
    "mean_frequency = frequencies.mean()\n",
    "plt.axhline(mean_frequency, color='r', linestyle='--', label='Frecuencia media')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'La frecuencia media de los valores de azimut es {mean_frequency:.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d882174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un dataframe por cada azimuth\n",
    "list_azimuths = []\n",
    "for n in range(360): \n",
    "    df_mask = dataset_valid['Azimuth'] == n\n",
    "    list_azimuths.append(dataset_valid[df_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cuenta la cantidad de datos que hay cada azimuth e imprimo (valor del azimuth, cantidad, )\n",
    "azimuth_quantity = []\n",
    "for n in range(360): \n",
    "    azimuth_quantity.append(len(list_azimuths[n]))\n",
    "    \n",
    "#azimuth_quantity.index(max(azimuth_quantity)),max(azimuth_quantity)\n",
    "azimuth_quantity\n",
    "for indice, cantidad in enumerate(azimuth_quantity):\n",
    "    print(f\"Azimuth {indice}: {cantidad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f8b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_azimuths[250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef96802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploteo el evento\n",
    "list_azimuths[250].iloc[0]['RCC_OBSPY'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "container_path= \"C:\\\\TESIS\\\\data_preprocessing\\\\SNR_Datasets_final\\\\\"\n",
    "dataset_valid.to_pickle(container_path +'dataset_valid.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1b0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui comienza la 3ra notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74075ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codigo para crear , los datos de entrada X1, X2 y de salida y1=cos(azimuth) , y2 = sin(azimuth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1859cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importa el conjunto de datos \n",
    "dataset = dataset_valid.copy()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5757ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a139183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hace un histograma de la variable a predecir \"azimut en el DataSet \"\n",
    "frequencies, bins, _ = plt.hist(dataset['Azimuth'], bins=300)\n",
    "plt.xlabel('Azimut')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "# Se calcula el valor medio de las frecuencias\n",
    "mean_frequency = frequencies.mean()\n",
    "plt.axhline(mean_frequency, color='r', linestyle='--', label='Frecuencia media')\n",
    "#plt.title('Distribución de la variable a predecir en el Conjunto de Entrenamiento')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'La frecuencia media de los valores de azimut es {mean_frequency:.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Me quedo con los azimuth en el rango 90- 270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6376a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino los eventos con Azimuth 0 a 90\n",
    "\n",
    "indexNames = dataset[ ((dataset['Azimuth'] < 90))].index\n",
    "dataset.drop(indexNames , inplace=True)\n",
    "#sns.pairplot(dataset[['Azimuth','Distance','Mc','Depth']], diag_kind=\"kde\")\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fdecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino los eventos con Azimuth 270 a 359\n",
    "\n",
    "indexNames = dataset[ ((dataset['Azimuth'] > 270))].index\n",
    "dataset.drop(indexNames , inplace=True)\n",
    "#sns.pairplot(dataset[['Azimuth','Distance','Mc','Depth']], diag_kind=\"kde\")\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede268be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino los Azimuts de los que se tiene menos de dos muestras\n",
    "class_counts = dataset[\"Azimuth\"].value_counts()\n",
    "classes_to_remove = class_counts[class_counts < 2].index\n",
    "filtered_dataset = dataset[~dataset[\"Azimuth\"].isin(classes_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20fe45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ef8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las variables para la estratificación\n",
    "features = [\"Azimuth\", \"Mc\", \"Distance\", \"Depth\"]\n",
    "\n",
    "# Dividir el dataframe en características (X) y variable objetivo (y)\n",
    "X = filtered_dataset.drop(\"Azimuth\", axis=1)\n",
    "y = filtered_dataset[\"Azimuth\"]\n",
    "\n",
    "# Realizar la estratificación utilizando StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_index, test_index = next(sss.split(X,y))\n",
    "\n",
    "# Obtener conjuntos de entrenamiento y prueba utilizando los índices generados\n",
    "X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd8a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_merged = X_train.join(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa84ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_merged = X_test.join(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be79458",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restablezco los indices del dataset eliminando las discontinuidades por las filas eliminadas\n",
    "dataset_test = dataset_test_merged.reset_index(drop=True)\n",
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70bb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restablezco los indices del dataset eliminando las discontinuidades por las filas eliminadas\n",
    "dataset_train = dataset_train_merged.reset_index(drop=True)\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15427d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hace un histograma de la variable a predecir \"azimut en el Set de Training\"\n",
    "frequencies, bins, _ = plt.hist(dataset_train['Azimuth'], bins=280)\n",
    "plt.xlabel('Azimut')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "# Se calcula el valor medio de las frecuencias\n",
    "mean_frequency = frequencies.mean()\n",
    "plt.axhline(mean_frequency, color='r', linestyle='--', label='Frecuencia media')\n",
    "plt.title('Distribución de la variable a predecir en el Conjunto de Entrenamiento')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(f'La frecuencia media de los valores de azimut es {mean_frequency:.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c32e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hace un histograma de la variable a predecir \"azimut en el set de Test\"\n",
    "frequencies, bins, _ = plt.hist(dataset_test['Azimuth'], bins=280)\n",
    "plt.xlabel('Azimut')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "# Se calcula el valor medio de las frecuencias\n",
    "mean_frequency = frequencies.mean()\n",
    "plt.axhline(mean_frequency, color='r', linestyle='--', label='Frecuencia media')\n",
    "plt.title('Distribución de la variable a predecir en el Conjunto de Validacion')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f'La frecuencia media de los valores de azimut es {mean_frequency:.2f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se muestra correlacion en los datos en el dataset de Train\n",
    "sns.pairplot(dataset_train[['Azimuth','Distance','Mc','Depth', ]], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e78a681",
   "metadata": {},
   "source": [
    "# Se muestra correlacion en los datos en el dataset de Test\n",
    "sns.pairplot(dataset_test[['Azimuth','Distance','Mc','Depth', ]], diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53541a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para conocer la composicion del dataset de Validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68831a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un dataframe por cada azimuth\n",
    "list_azimuths_test = []\n",
    "for n in range(360): \n",
    "    df_mask_test = dataset_test['Azimuth'] == n\n",
    "    list_azimuths_test.append(dataset_test[df_mask_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93abe89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cuenta la cantidad de datos que hay cada azimuth e imprimo (valor del azimuth, cantidad, )\n",
    "azimuth_quantity_test = []\n",
    "for n in range(360): \n",
    "    azimuth_quantity_test.append(len(list_azimuths_test[n]))\n",
    "#azimuth_quantity.index(max(azimuth_quantity)),max(azimuth_quantity)\n",
    "for indice, cantidad in enumerate(azimuth_quantity_test):\n",
    "    print(f\"Azimuth {indice}: {cantidad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777c13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un dataframe por cada azimuth del dataset Train\n",
    "list_azimuths_train = []\n",
    "for n in range(360): \n",
    "    df_mask_train = dataset_train['Azimuth'] == n\n",
    "    list_azimuths_train.append(dataset_train[df_mask_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8a3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se cuenta la cantidad de datos que hay cada azimuth e imprimo (valor del azimuth, cantidad, )\n",
    "azimuth_quantity_train = []\n",
    "for n in range(360): \n",
    "    azimuth_quantity_train.append(len(list_azimuths_train[n]))\n",
    "for indice, cantidad in enumerate(azimuth_quantity_train):\n",
    "    print(f\"Azimuth {indice}: {cantidad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se conforman el vector de entrada de Training X1_train, \n",
    "#Se escoge la ventana de tiempo a utilizar, \n",
    "#Se forma la entrada2 Matriz de Covarianza, Vector Propio y Valor Propio \n",
    "\n",
    "window_width=500\n",
    "samples_number=len(dataset_train)\n",
    "X1_train=np.empty(shape=[0,window_width,3])\n",
    "for n in range(samples_number): \n",
    "    print('Procesando :',n)\n",
    "    tmp=dataset_train.iloc[n]\n",
    "    z_original= tmp['Z_data']    \n",
    "    n_original= tmp['N_data']    \n",
    "    e_original= tmp['E_data'] \n",
    "    z_max = np.max(z_original)\n",
    "    n_max = np.max(n_original)\n",
    "    e_max = np.max(e_original)\n",
    "    print('z_max :',z_max)\n",
    "    print('n_max :',n_max)\n",
    "    print('e_max :',e_max)\n",
    "    global_max = z_max\n",
    "    if global_max < n_max :\n",
    "        global_max = n_max\n",
    "    if global_max < e_max :\n",
    "        global_max = e_max\n",
    "    print('global_max :',global_max)\n",
    "    z_norm = z_original/global_max\n",
    "    n_norm = n_original/global_max\n",
    "    e_norm = e_original/global_max    \n",
    "    new_z= z_norm[0: window_width]    \n",
    "    new_n= n_norm[0: window_width]\n",
    "    new_e= e_norm[0: window_width]    \n",
    "    zne=np.dstack((new_z,new_n,new_e)) \n",
    "    X1_train=np.append(X1_train,zne, axis=0)     \n",
    "print(X1_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se conforman los vectores de entrada de Test X1_test y X2_test, \n",
    "#Se escoge la ventana de tiempo a utilizar, \n",
    "#Se forma la entrada2 Matriz de Covarianza, Vector Propio y Valor Propio \n",
    "\n",
    "window_width=500\n",
    "samples_number=len(dataset_test)\n",
    "X1_test=np.empty(shape=[0,window_width,3])\n",
    "for n in range(samples_number): \n",
    "    print('Procesando :',n)\n",
    "    tmp=dataset_test.iloc[n]\n",
    "    z_original= tmp['Z_data']    \n",
    "    n_original= tmp['N_data']    \n",
    "    e_original= tmp['E_data'] \n",
    "    z_max = np.max(z_original)\n",
    "    n_max = np.max(n_original)\n",
    "    e_max = np.max(e_original)\n",
    "    print('z_max :',z_max)\n",
    "    print('n_max :',n_max)\n",
    "    print('e_max :',e_max)\n",
    "    global_max = z_max\n",
    "    if global_max < n_max :\n",
    "        global_max = n_max\n",
    "    if global_max < e_max :\n",
    "        global_max = e_max\n",
    "    print('global_max :',global_max)\n",
    "    z_norm = z_original/global_max\n",
    "    n_norm = n_original/global_max\n",
    "    e_norm = e_original/global_max    \n",
    "    new_z= z_norm[0: window_width]    \n",
    "    new_n= n_norm[0: window_width]\n",
    "    new_e= e_norm[0: window_width]    \n",
    "    zne=np.dstack((new_z,new_n,new_e)) \n",
    "    X1_test=np.append(X1_test,zne, axis=0)    \n",
    "print(X1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5a1ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se conforman los vectores de salida de Training, \n",
    "\n",
    "samples_number=len(dataset_train)\n",
    "y_train_cos=np.empty(shape=[samples_number])\n",
    "y_train_sin=np.empty(shape=[samples_number])\n",
    "for n in range(samples_number):   \n",
    "    tmp=dataset_train.iloc[n]    \n",
    "    \n",
    "    azimuth= tmp['Azimuth']\n",
    "    if azimuth !=0 and azimuth !=90 and azimuth !=180 and azimuth !=270 :\n",
    "        y_train_cos[n]= math.cos(math.radians(azimuth))\n",
    "        y_train_sin[n]= math.sin(math.radians(azimuth))\n",
    "    else:\n",
    "        if azimuth == 0:\n",
    "            y_train_cos[n]=1\n",
    "            y_train_sin[n]=0\n",
    "        if azimuth == 90:\n",
    "            y_train_cos[n]=0\n",
    "            y_train_sin[n]=1\n",
    "        if azimuth == 180:\n",
    "            y_train_cos[n]=-1\n",
    "            y_train_sin[n]=0\n",
    "        if azimuth == 270:\n",
    "            y_train_cos[n]=0\n",
    "            y_train_sin[n]=-1    \n",
    "print('y_train_cos',y_train_cos)\n",
    "print('y_train_sin',y_train_sin)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d938c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_train_cos,y_train_sin )\n",
    "plt.axis('square')\n",
    "plt.xlabel('Cos(azimut)')\n",
    "plt.ylabel('Sen(azimuth)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60adee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se conforman los vectores de salida de Test Version \n",
    "\n",
    "samples_number=len(dataset_test)\n",
    "y_test_cos=np.empty(shape=[samples_number])\n",
    "y_test_sin=np.empty(shape=[samples_number])\n",
    "for n in range(samples_number):   \n",
    "    tmp=dataset_test.iloc[n]\n",
    "    azimuth= tmp['Azimuth']\n",
    "    if azimuth !=0 and azimuth !=90 and azimuth !=180 and azimuth !=270 :\n",
    "        y_test_cos[n]= math.cos(math.radians(azimuth))\n",
    "        y_test_sin[n]= math.sin(math.radians(azimuth))\n",
    "    else:\n",
    "        if azimuth == 0:\n",
    "            y_test_cos[n]=1\n",
    "            y_test_sin[n]=0\n",
    "        if azimuth == 90:\n",
    "            y_test_cos[n]=0\n",
    "            y_test_sin[n]=1\n",
    "        if azimuth == 180:\n",
    "            y_test_cos[n]=-1\n",
    "            y_test_sin[n]=0\n",
    "        if azimuth == 270:\n",
    "            y_test_cos[n]=0\n",
    "            y_test_sin[n]=-1    \n",
    "print('y_test_cos',y_test_cos)\n",
    "print('y_test_sin',y_test_sin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed23847c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test_cos, y_test_sin)\n",
    "plt.axis('square')\n",
    "plt.xlabel('Cos(azimut)')\n",
    "plt.ylabel('Sen(azimuth)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9603732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvo los vectores de entrada  y salida de Training\n",
    "container_path= \"C:\\\\TESIS\\\\FINAL\\\\Training\\\\Preprocesing\\\\Input_Output_Vectors\\\\\"\n",
    "np.save(container_path + 'X1_train.npy',X1_train)\n",
    "np.save(container_path + 'y_train_cos.npy',y_train_cos)\n",
    "np.save(container_path + 'y_train_sin.npy',y_train_sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758b94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvo los vectores de entrada  y salida de Test\n",
    "np.save(container_path + 'X1_test.npy',X1_test)\n",
    "np.save(container_path + 'y_test_cos.npy',y_test_cos)\n",
    "np.save(container_path + 'y_test_sin.npy',y_test_sin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvo los dataset de Training y de Test\n",
    "dataset_train.to_pickle(container_path +'dataset_train.pkl')\n",
    "dataset_test.to_pickle(container_path +'dataset_test.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e4f11c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
